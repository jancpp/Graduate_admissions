{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Graduate Admissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing and exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = \"./data/Admission_Predict_Ver1.1.csv\"\n",
    "df = pd.read_csv(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "- gre_score\n",
    "- toefl_score\n",
    "- university_rating\n",
    "- sop\n",
    "- lor\n",
    "- cgpa\n",
    "- research\n",
    "\n",
    "### Target\n",
    "- chance_of_admit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "df.columns = map(str.lower, df.columns)\n",
    "df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "df = df.rename(index=str, columns={\"lor_\": \"lor\", \"chance_of_admit_\": \"chance_of_admit\"});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping \"serial_no.\" as it is not needed\n",
    "df = df.drop(columns = ['serial_no.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(r'data/df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a pairplot to visualize distribution of data and correlation among columns\n",
    "- As we can see - toefl_score, gre_score and cgpa have linear relationship among themselves and with target chance_of_admit\n",
    "- That means people who scored higher in gre, also scored higher in toefl and vice versa\n",
    "- People with higher cgpa score higher in gre and toefl\n",
    "- higher gre & toelf score means higher chance of admit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(df, hue = 'chance_of_admit') # uncommented to speed up compiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix\n",
    "- Darker squares means higher correlation\n",
    "- Best case scenario is having  many features having higher correlation with target, but lower correlation among themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,len(df.columns),1)\n",
    "ax.set_xticks(ticks)\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(df.columns)\n",
    "ax.set_yticklabels(df.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data to independent variables matrix (X) and dependent variable vector (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, 7].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of utility functions to plot the results from each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(y_true,y_pred):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    y_true.hist(bins=22, normed=True)\n",
    "    y_pred.hist(bins=22, normed=True, alpha=0.5)\n",
    "    \n",
    "    \n",
    "def plot_best_fit(y_true, y_pred, model_name):\n",
    "    plt.figure(figsize=(12,8))\n",
    "#     reorder = y_true.sort.index.tolist()\n",
    "#     x_series = np.array(y_true[reorder])\n",
    "#     y_series = np.array(y_pred[reorder])    \n",
    "    x_series = np.array(y_true)\n",
    "    y_series = np.array(y_pred)\n",
    "    \n",
    "    # Create scatter plot of \n",
    "    plt.xlim(0,1.1)\n",
    "    plt.ylim(0,1.1)\n",
    "    plt.scatter(x_series, y_series, s=30, alpha=0.2, marker='o')\n",
    "\n",
    "    # Create line of best fit and confidence intervals\n",
    "    par = np.polyfit(x_series, y_series, 1, full=True)\n",
    "    slope=par[0][0]\n",
    "    intercept=par[0][1]\n",
    "    xl = [min(x_series), max(x_series)]\n",
    "    yl = [slope*x + intercept for x in xl]\n",
    "\n",
    "    # Determine error bounds\n",
    "    yerr = [abs(slope*xx + intercept - yy) for xx,yy in zip(x_series,y_series)]\n",
    "    par = np.polyfit(x_series, yerr, 2, full=True)\n",
    "    yerrUpper = [(xx*slope+intercept)+(par[0][0]*xx**2 + par[0][1]*xx + par[0][2]) for xx,yy in zip(x_series,y_series)]\n",
    "    yerrLower = [(xx*slope+intercept)-(par[0][0]*xx**2 + par[0][1]*xx + par[0][2]) for xx,yy in zip(x_series,y_series)]\n",
    "\n",
    "    plt.plot(xl, yl, 'r')\n",
    "    plt.plot(x_series, yerrLower, '--r')\n",
    "    plt.plot(x_series, yerrUpper, '--r')\n",
    "\n",
    "    plt.xlabel(\"Actual Admissions\")\n",
    "    plt.ylabel(\"{} Predictoins\".format(model_name))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "\n",
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "X_train_lin_reg = X_train\n",
    "y_train_lin_reg = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal function that outputs the final optimized theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y, alpha, num_iters):\n",
    "    n = X.shape[1]\n",
    "    one_column = np.ones((X.shape[0],1))\n",
    "    X = np.concatenate((one_column, X), axis = 1)\n",
    "    # initializing the parameter vector...\n",
    "    theta = np.zeros(n+1)\n",
    "    # hypothesis calculation....\n",
    "    h = hypothesis(theta, X, n)\n",
    "    # returning the optimized parameters by Gradient Descent...\n",
    "    theta, cost = gradient_descent(theta,alpha,num_iters,h,X,y,n)\n",
    "    return theta, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that calculates and outputs the hypothesis value of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(theta, X, n):\n",
    "    h = np.ones((X.shape[0],1))\n",
    "    theta = theta.reshape(1,n+1)\n",
    "    for i in range(0,X.shape[0]):\n",
    "        h[i] = float(np.matmul(theta, X[i]))\n",
    "    h = h.reshape(X.shape[0])\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that performs the gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(theta, alpha, num_iters, h, X, y, n):\n",
    "    cost = np.ones(num_iters)\n",
    "    for i in range(0,num_iters):\n",
    "        theta[0] = theta[0] - (alpha/X.shape[0]) * sum(h - y)\n",
    "        for j in range(1,n+1):\n",
    "            theta[j] = theta[j] - (alpha/X.shape[0]) * sum((h-y) * X.transpose()[j])\n",
    "        h = hypothesis(theta, X, n)\n",
    "        cost[i] = (1/X.shape[0]) * 0.5 * sum(np.square(h - y))\n",
    "    theta = theta.reshape(1,n+1)\n",
    "    return theta, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train = np.ones(X_train_lin_reg.shape[1])\n",
    "std_train = np.ones(X_train_lin_reg.shape[1])\n",
    "for i in range(0, X_train_lin_reg.shape[1]):\n",
    "    mean_train[i] = np.mean(X_train_lin_reg.transpose()[i])\n",
    "    std_train[i] = np.std(X_train_lin_reg.transpose()[i])\n",
    "    for j in range(0, X_train_lin_reg.shape[0]):\n",
    "        X_train_lin_reg[j][i] = (X_train_lin_reg[j][i] - mean_train[i])/std_train[i]   \n",
    "        \n",
    "mean_test = np.ones(X_test.shape[1])\n",
    "std_test = np.ones(X_test.shape[1])\n",
    "for i in range(0, X_test.shape[1]):\n",
    "    mean_test[i] = np.mean(X_test.transpose()[i])\n",
    "    std_test[i] = np.std(X_test.transpose()[i])\n",
    "    for j in range(0, X_test.shape[0]):\n",
    "        X_test[j][i] = (X_test[j][i] - mean_test[i])/std_test[i]      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the principal function with learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 1000\n",
    "theta, cost = linear_regression(X_train_lin_reg, y_train_lin_reg, 0.01, num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduction in the cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "cost = list(cost)\n",
    "n_iterations = [x for x in range(1,num_iters+1)]\n",
    "plt.plot(n_iterations, cost)\n",
    "plt.xlabel('No. of iterations')\n",
    "plt.ylabel('Cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lin_reg = np.concatenate((np.ones((X_test.shape[0],1)), X_test) ,axis = 1)\n",
    "predictions_lin_reg = hypothesis(theta, X_test_lin_reg, X_test_lin_reg.shape[1] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(metrics.mean_squared_error(y_test, predictions_lin_reg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot predictions and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test.ravel(), '-', predictions_lin_reg, '-')\n",
    "plt.title('Linear regression')\n",
    "plt.xlabel('Number of Test Samples') \n",
    "plt.ylabel('Chance of Admission')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accurency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_error = y_test - predictions_lin_reg\n",
    "\n",
    "error_lin_reg = np.mean(np.abs(output_error))\n",
    "accuracy_lin_reg = (1 - error_lin_reg) * 100\n",
    "\n",
    "print(\"Test Accuracy \" + str(round(accuracy_lin_reg,2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_col_order = ['ChanceOfAdmit', 'Predicted', 'Delta']\n",
    "result_lin_reg = pd.DataFrame({'ChanceOfAdmit':y_test,\n",
    "                               'Predicted':predictions_lin_reg,\n",
    "                               'Delta': abs(y_test - predictions_lin_reg)})\n",
    "                        \n",
    "result_lin_reg[output_col_order].to_csv('results/LinearRegression.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose 'gre score', 'toefl score', and 'cgpa' columns that seemed the most important in getting admitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lin_reg_fe1 = df[['gre_score', 'toefl_score', 'cgpa']]\n",
    "X_lin_reg_fe1 = X_lin_reg_fe1.iloc[:,:].values\n",
    "y_lin_reg_fe1 = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_lin_reg_fe1, X_test_lin_reg_fe1, y_train_lin_reg_fe1, y_test_lin_reg_fe1 = train_test_split(X_lin_reg_fe1, y_lin_reg_fe1, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_fe1 = np.ones(X_train_lin_reg_fe1.shape[1])\n",
    "std_train_fe1 = np.ones(X_train_lin_reg_fe1.shape[1])\n",
    "for i in range(0, X_train_lin_reg_fe1.shape[1]):\n",
    "    mean_train_fe1[i] = np.mean(X_train_lin_reg_fe1.transpose()[i])\n",
    "    std_train_fe1[i] = np.std(X_train_lin_reg_fe1.transpose()[i])\n",
    "    for j in range(0, X_train_lin_reg_fe1.shape[0]):\n",
    "        X_train_lin_reg_fe1[j][i] = (X_train_lin_reg_fe1[j][i] - mean_train_fe1[i])/std_train_fe1[i]\n",
    "        \n",
    "mean_test_fe1 = np.ones(X_test_lin_reg_fe1.shape[1])\n",
    "std_test = np.ones(X_test_lin_reg_fe1.shape[1])\n",
    "for i in range(0, X_test_lin_reg_fe1.shape[1]):\n",
    "    mean_test[i] = np.mean(X_test_lin_reg_fe1.transpose()[i])\n",
    "    std_test[i] = np.std(X_test_lin_reg_fe1.transpose()[i])\n",
    "    for j in range(0, X_test_lin_reg_fe1.shape[0]):\n",
    "        X_test_lin_reg_fe1[j][i] = (X_test_lin_reg_fe1[j][i] - mean_test[i])/std_test[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the principal function with learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 1000\n",
    "theta_reg_fe1, cost_reg_fe1 = linear_regression(X_train_lin_reg_fe1, y_train_lin_reg_fe1, 0.01, num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lin_reg_fe1 = np.concatenate((np.ones((X_test_lin_reg_fe1.shape[0],1)), X_test_lin_reg_fe1) ,axis = 1)\n",
    "predictions_lin_reg_fe1 = hypothesis(theta_reg_fe1, X_test_lin_reg_fe1, X_test_lin_reg_fe1.shape[1] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(metrics.mean_squared_error(y_test, predictions_lin_reg_fe1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test.ravel(), '-', predictions_lin_reg_fe1, '-')\n",
    "plt.title('Linear regression')\n",
    "plt.xlabel('Number of Test Samples') \n",
    "plt.ylabel('Chance of Admission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_error_fe1 = y_test - predictions_lin_reg_fe1\n",
    "\n",
    "error_lin_reg_fe1 = np.mean(np.abs(output_error_fe1))\n",
    "accuracy_lin_reg_fe1 = (1 - error_lin_reg_fe1) * 100\n",
    "\n",
    "print(\"Test Accuracy \" + str(round(accuracy_lin_reg_fe1,2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering # 2\n",
    "\n",
    "We chose only 'gre score', and 'cgpa' columns that seemed the most important in getting admitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lin_reg_fe2 = df[['gre_score', 'cgpa']]\n",
    "X_lin_reg_fe2 = X_lin_reg_fe2.iloc[:,:].values\n",
    "y_lin_reg_fe2 = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_lin_reg_fe2, X_test_lin_reg_fe2, y_train_lin_reg_fe2, y_test_lin_reg_fe2 = train_test_split(X_lin_reg_fe2, y_lin_reg_fe2, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_fe2 = np.ones(X_train_lin_reg_fe2.shape[1])\n",
    "std_train_fe2 = np.ones(X_train_lin_reg_fe2.shape[1])\n",
    "for i in range(0, X_train_lin_reg_fe2.shape[1]):\n",
    "    mean_train_fe2[i] = np.mean(X_train_lin_reg_fe2.transpose()[i])\n",
    "    std_train_fe2[i] = np.std(X_train_lin_reg_fe2.transpose()[i])\n",
    "    for j in range(0, X_train_lin_reg_fe2.shape[0]):\n",
    "        X_train_lin_reg_fe2[j][i] = (X_train_lin_reg_fe2[j][i] - mean_train_fe2[i])/std_train_fe2[i]\n",
    "        \n",
    "mean_test_fe2 = np.ones(X_test_lin_reg_fe2.shape[1])\n",
    "std_test = np.ones(X_test_lin_reg_fe2.shape[1])\n",
    "for i in range(0, X_test_lin_reg_fe2.shape[1]):\n",
    "    mean_test[i] = np.mean(X_test_lin_reg_fe2.transpose()[i])\n",
    "    std_test[i] = np.std(X_test_lin_reg_fe2.transpose()[i])\n",
    "    for j in range(0, X_test_lin_reg_fe2.shape[0]):\n",
    "        X_test_lin_reg_fe2[j][i] = (X_test_lin_reg_fe2[j][i] - mean_test[i])/std_test[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the principal function with learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 1000\n",
    "theta_reg_fe2, cost_reg_fe2 = linear_regression(X_train_lin_reg_fe2, y_train_lin_reg_fe2, 0.01, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "cost_reg_fe2 = list(cost_reg_fe2)\n",
    "n_iterations = [x for x in range(1,num_iters+1)]\n",
    "plt.plot(n_iterations, cost_reg_fe2)\n",
    "plt.xlabel('No. of iterations')\n",
    "plt.ylabel('Cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lin_reg_fe2 = np.concatenate((np.ones((X_test_lin_reg_fe2.shape[0],1)), X_test_lin_reg_fe2) ,axis = 1)\n",
    "predictions_lin_reg_fe2 = hypothesis(theta_reg_fe2, X_test_lin_reg_fe2, X_test_lin_reg_fe2.shape[1] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(metrics.mean_squared_error(y_test, predictions_lin_reg_fe2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test.ravel(), '-', predictions_lin_reg_fe2, '-')\n",
    "plt.title('Linear regression')\n",
    "plt.xlabel('Number of Test Samples') \n",
    "plt.ylabel('Chance of Admission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_error_fe2 = y_test - predictions_lin_reg_fe2\n",
    "\n",
    "error_lin_reg_fe2 = np.mean(np.abs(output_error_fe2))\n",
    "accuracy_lin_reg_fe2 = (1 - error_lin_reg_fe2) * 100\n",
    "\n",
    "print(\"Test Accuracy \" + str(round(accuracy_lin_reg_fe2,2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
